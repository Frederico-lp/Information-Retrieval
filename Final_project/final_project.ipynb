{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-image library for image processing tools in python\n",
    "!pip3 install scikit-image\n",
    "# Scikit-learn library for ML in python\n",
    "!pip3 install scikit-learn\n",
    "# install the library h5py (needed to load and read the ground truth relevance file of assignment 02)\n",
    "!pip3 install h5py\n",
    "# install the library tqdm (for progress visualization)\n",
    "!pip3 install tqdm\n",
    "# Matplotlib/Pylab for plots and visualization in python\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try:\n",
    "- use of different keypoint descriptors (e.g. SIFT, SURF, and so on) \n",
    "- study the impact of different clustering techniques to learn the visual dictionary (e.g. fuzzy k-means, soft k-means, hierarchical clustering) of different sizes of the vocabulary (values of k)\n",
    "- use of different distance metrics to compare the final descriptors \n",
    "- use of Machine Learning for learning to rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the query and map images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# map\n",
    "with open(\"data02/database/database.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"data02/query/query.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the relevance judgements\n",
    "with h5py.File(\"data02/london_gt.h5\",\"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim.shape)\n",
    "index = []\n",
    "new_sim = []\n",
    "for i in range(len(sim)):\n",
    "    if np.sum(sim, axis=1)[i] != 0:\n",
    "        new_sim.append(sim[i])\n",
    "        \n",
    "sim = np.array(new_sim)\n",
    "print(sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sim)):\n",
    "    if np.sum(sim, axis=1)[i] == 0:\n",
    "        for k in range(len(sim[i])):\n",
    "            if sim[i][k] == 1:\n",
    "                print(\"Still queries left with 0 relevant images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random index for the query image\n",
    "query_idx = np.random.randint(0, 499) \n",
    "\n",
    "# select the relevant and non-relevant map images for the randomly selected query image\n",
    "rel = np.where(sim[query_idx, :] == 1)\n",
    "nonrel = np.where(sim[query_idx, :] == 0)\n",
    "\n",
    "\n",
    "# randomly select a relevant and non-relevant image\n",
    "rel_idx = rel[0][np.random.randint(0, len(rel[0]) - 1)]\n",
    "nonrel_idx = nonrel[0][np.random.randint(0, len(nonrel[0]) - 1)]\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(plt.imread('data02/' + q_imgs[query_idx]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('Relevant image result (from map)')\n",
    "plt.imshow(plt.imread('data02/' + m_imgs[rel_idx]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Non-relevant image result (from map)')\n",
    "plt.imshow(plt.imread('data02/' + m_imgs[nonrel_idx]))\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "#from skimage.feature import ORB\n",
    "from skimage.feature import SIFT\n",
    "from skimage.color import rgb2gray\n",
    "import cv2 \n",
    "\n",
    "# Initialize the descriptor\n",
    "\n",
    "#descriptor_extractor = SIFT()\n",
    "descriptor_extractor = cv2.SIFT_create()\n",
    "#descriptor_extractor = ORB(n_keypoints=50)\n",
    "# Initialize the data structure that will contain all the descriptors\n",
    "descriptors = None\n",
    "\n",
    "# Loop over map images\n",
    "for img_name in m_imgs:\n",
    "    img = plt.imread(os.path.join('data02/', img_name))\n",
    "    #img = rgb2gray(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Extract descriptors\n",
    "    keypoints_surf, descriptors_img = descriptor_extractor.detectAndCompute(img, None)\n",
    "    \n",
    "    # Accumulate the computed descriptors\n",
    "    if descriptors is None:\n",
    "        descriptors = descriptors_img\n",
    "    else:\n",
    "        descriptors = np.vstack( (descriptors, descriptors_img))\n",
    "    \n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save descriptors (uncomment if you want to save the computed descriptors)\n",
    "f = open('data02/SIFT-full-descriptors-map.bin', 'wb')\n",
    "data = pickle.dump(descriptors, f)\n",
    "f.close()\n",
    "\n",
    "# load pre-computed descriptors\n",
    "f = open('data02/SIFT-full-descriptors-map.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnanny import verbose\n",
    "from cv2 import sqrt\n",
    "import sklearn\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# clustering\n",
    "K = 100  # number of clusters (equivalent to the number of words) we want to estimate\n",
    "num_initialization = 5 # Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "\n",
    "# Run the k-means clustering\n",
    "kmeans = MiniBatchKMeans(n_clusters=K, random_state=0, n_init=num_initialization, verbose=1)\n",
    "clusters = kmeans.fit(descriptors)  # we use the descriptors extracted from the map (training) images before\n",
    "centroids = clusters.cluster_centers_\n",
    "\n",
    "\n",
    "print(\"Shape of the centroids matrix: \", centroids.shape)\n",
    "print(\"We computed \", centroids.shape[0], \"centroids of lengh \", centroids.shape[1], \" (the same of the descriptor)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save/load the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save centroids (uncomment if you want to save the computed centroids)\n",
    "f = open('data02/SIFT-full-mini-batch-centroids-map.bin', 'wb')\n",
    "data = pickle.dump(centroids, f)\n",
    "f.close()\n",
    "\n",
    "# load pre-computed centroids\n",
    "f = open('data02/SIFT-full-mini-batch-centroids-map.bin', 'rb')\n",
    "centroids = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import skimage\n",
    "from skimage.feature import SIFT\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# compute the bag of word vector for an image\n",
    "def bag_of_words(centroids, img_descriptors):\n",
    "    n_centroids = centroids.shape[0]  # number of centroids found with the KMeans clustering\n",
    "    n_descriptors = img_descriptors.shape[0]  # number of descriptors extracted from the image\n",
    "    \n",
    "    # initialization of the bag of words (BoW) vector\n",
    "    # Note that the BoW vector has length equal to the number of cluster centroids\n",
    "    # The cluster centroids are indeed our visual words, and the BoW will be the histogram of these words found in the given image\n",
    "    bow_vector = np.zeros(n_centroids)  \n",
    "    \n",
    "    for i in range(n_descriptors):\n",
    "        closest_dist = 9999\n",
    "        index = 0\n",
    "        for j in range(n_centroids):\n",
    "            #dist = np.linalg.norm(img_descriptors[i]-centroids[j])\n",
    "            dist = distance.cityblock(img_descriptors[i],centroids[j])\n",
    "            if(dist < closest_dist):\n",
    "                closest_dist = dist\n",
    "                index = j\n",
    "        bow_vector[index] += 1\n",
    "    return bow_vector\n",
    "\n",
    "\n",
    "# Test the implementation of the BoW vector computation\n",
    "img = plt.imread(os.path.join('data02/', q_imgs[0]))\n",
    "#img = rgb2gray(img)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "keypoints_surf, query_img_descriptors = descriptor_extractor.detectAndCompute(img, None)\n",
    "\n",
    "bow = bag_of_words(centroids, query_img_descriptors)\n",
    "print(\"Size of the bow vector: \", bow.shape)\n",
    "print(\"Bow vector: \", bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the images to BoW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_map_images = None\n",
    "# loop over the images in the map set\n",
    "for img_name in tqdm(m_imgs):\n",
    "    # load image\n",
    "    img = plt.imread(os.path.join('data02/', img_name))\n",
    "    #img = rgb2gray(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # extract the keypoints and corresponding descriptors\n",
    "    _,img_descriptors = descriptor_extractor.detectAndCompute(img, None)# descriptors (the feature vectors)\n",
    "    \n",
    "    # compute BoW representation of the image (using the basic 'words', i.e. centroids, computed earlier)\n",
    "    bow = bag_of_words(centroids, img_descriptors)\n",
    "    # add the computed BoW vector to the set of map representations\n",
    "    if bow_map_images is None:\n",
    "        bow_map_images = bow\n",
    "    else:\n",
    "        bow_map_images = np.vstack( (bow_map_images, bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "orig_bow_map_images = bow_map_images\n",
    "\n",
    "# Compute z-score statistics\n",
    "scaler = preprocessing.StandardScaler().fit(bow_map_images)\n",
    "# Normalize the vectors of the map collection (0 mean and 1 std)\n",
    "bow_map_images = scaler.transform(bow_map_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "# receives as input the:\n",
    "#   - bag of words vectors of the map images\n",
    "#   - the bag of work vector of the query image\n",
    "def retrieve_images(map_bow_vectors, query_bow):\n",
    "    n_map_bow_vectors = map_bow_vectors.shape[0]\n",
    "    bow_distances = np.zeros(n_map_bow_vectors)\n",
    "    most_similar = None  # use this to \n",
    "    \n",
    "    for i in range(n_map_bow_vectors):\n",
    "        #dist = np.linalg.norm(map_bow_vectors[i]-query_bow)\n",
    "        dist = distance.cityblock(map_bow_vectors[i],query_bow)\n",
    "        #dist = distance.cityblock(map_bow_vectors[i],query_bow)\n",
    "        bow_distances[i] = dist\n",
    "    \n",
    "    most_similar = np.argsort(bow_distances, axis=- 1, kind=None, order=None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return most_similar\n",
    "\n",
    "\n",
    "        #dist = distance(map_bow_vectors[i]-query_bow)\n",
    "\n",
    "# Retrieve the most similar images to query image 221 (index 221-1=220)\n",
    "query_idx = 220\n",
    "img = plt.imread(\"data02/\" + q_imgs[query_idx])\n",
    "#img = rgb2gray(img)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# compute bag of words\n",
    "_,query_img_descriptors = descriptor_extractor.detectAndCompute(img, None)\n",
    "\n",
    "bow = bag_of_words(centroids, query_img_descriptors)\n",
    "\n",
    "# Normalize the query BoW vector using the mean and variance of the map (computed earlier and saved into the scaler object)\n",
    "bow = scaler.transform(bow.reshape(-1, 1).transpose())\n",
    "bow = bow.transpose().reshape(-1)\n",
    "\n",
    "# Retrieve the indices of the top-10 similar images from the map\n",
    "retrieved_images = retrieve_images(bow_map_images, bow)\n",
    "print('Indices of similar images retrieved: ', retrieved_images[:10])\n",
    "# Indices of the relevant map images for the query: we have the relevance judgements (Ground truth)\n",
    "relevant_images = np.where(sim[query_idx, :] == 1)[0]\n",
    "print('Indices of relevant images (given in the GT relevance judgements): ', relevant_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision at k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(relevant, retrieved):\n",
    "    count = 0\n",
    "    if len(retrieved) > 0:\n",
    "        for x in relevant:\n",
    "            if x in retrieved:\n",
    "                count += 1\n",
    "        return count / len(retrieved)\n",
    "    return 0\n",
    "    \n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    kRetrieved = retrieved[0:k]\n",
    "    return precision(relevant, kRetrieved)\n",
    "\n",
    "prec5 = precision_at_k(relevant_images, retrieved_images, 5)\n",
    "prec10 = precision_at_k(relevant_images, retrieved_images, 10)\n",
    "\n",
    "print('P@5: ', prec5)\n",
    "print('P@10: ', prec10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevant, retrieved):\n",
    "    avsum = 0\n",
    "    for i in range(len(retrieved)):\n",
    "        if retrieved[i] in relevant:\n",
    "            pr = precision(relevant, retrieved[0:i+1])\n",
    "            avsum += pr\n",
    "    return avsum/len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = len(all_relevant)\n",
    "    for i in range(len(all_relevant)):\n",
    "        total += average_precision(all_relevant[i],all_retrieved[i])\n",
    "    return total / count\n",
    "\n",
    "relevant_images_MAP = []\n",
    "retrieved_images_MAP = []\n",
    "for i in range(len(sim)):\n",
    "    relevant_images_MAP.append(np.where(sim[i, :] == 1)[0])\n",
    "    \n",
    "    img = plt.imread(\"data02/\" + q_imgs[i])\n",
    "    #img = rgb2gray(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # compute bag of words\n",
    "    _,query_img_descriptors = descriptor_extractor.detectAndCompute(img, None)\n",
    "    bow = bag_of_words(centroids, query_img_descriptors)\n",
    "\n",
    "    # Normalize the query BoW vector using the mean and variance of the map (computed earlier and saved into the scaler object)\n",
    "    bow = scaler.transform(bow.reshape(-1, 1).transpose())\n",
    "    bow = bow.transpose().reshape(-1)\n",
    "\n",
    "    # Retrieve the indices of the top-10 similar images from the map\n",
    "    retrieved_images_MAP.append(retrieve_images(bow_map_images, bow))\n",
    "\n",
    "MAP = mean_average_precision(relevant_images_MAP, retrieved_images_MAP)\n",
    "print('The mean average precision is: ', MAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e511838c1e57aaa348af65028b1d0e807fc0ba78c128e77bcd38a63295926b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
